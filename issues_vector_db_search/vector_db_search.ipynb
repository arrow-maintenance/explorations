{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176b0ef7-4ef1-41fd-87b3-b6327e52c76a",
   "metadata": {},
   "source": [
    "# Vector DB Search \n",
    "\n",
    "This notebook can be used to do a vector DB search of Arrow issues.  This allows for semantic search—retrieving issues based on meaning rather than exact keyword matches. This differs from GitHub's built-in search, which is mostly lexical and relies on specific terms, labels, or filters. A vector DB can surface issues with similar intent or topic even if they use different wording, making it more useful for detecting duplicates, related bugs, or thematic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582fab66-3adf-4e9f-b9b6-c038d100073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nic/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785cb46b-04ee-4dc2-8d1d-e0cca0cf5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"../test_data/issues_min.json.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    df = json.load(f)\n",
    "    \n",
    "df = pd.DataFrame(df)\n",
    "data = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cfe1ff5-0de1-489f-b655-fbff48492b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://github.com/apache/arrow/pull/46723',\n",
       " 'title': 'GH-46214: [C++] Improve S3 client initialization',\n",
       " 'created_at': '2025-06-05T16:50:31Z',\n",
       " 'user_login': 'pitrou',\n",
       " 'labels': ['Component: C++', 'awaiting review'],\n",
       " 'created_at.1': '2025-06-05T16:50:31Z',\n",
       " 'closed_at': {},\n",
       " 'pull_request': 'https://api.github.com/repos/apache/arrow/pulls/46723',\n",
       " 'body': '### Rationale for this change\\r\\n\\r\\nThe default constructor of the `ClientConfiguration` class in the AWS SDK can issue spurious EC2 metadata requests to resolve the current region, even if we would later set the region from our S3 options.\\r\\n\\r\\n### What changes are included in this PR?\\r\\n\\r\\n1. Avoid spurious EC2 metadata calls by disabling \"IMDS\" in the `ClientConfiguration` constructor\\r\\n2. Change the smart defaults from \"legacy\" to \"standard\" (see https://docs.aws.amazon.com/sdkref/latest/guide/feature-smart-config-defaults.html)\\r\\n3. Let the user configure the smart defaults in `S3Options`\\r\\n\\r\\n### Are these changes tested?\\r\\n\\r\\nBy existing CI tests and configurations.\\r\\n\\r\\n### Are there any user-facing changes?\\r\\n\\r\\nThe default S3 settings are potentially changed. Hopefully this will not trigger any regression in behavior.\\r\\n\\n* GitHub Issue: #46214',\n",
       " 'state': 'open'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f2de7-915b-46ec-981f-e33ac46f32c0",
   "metadata": {},
   "source": [
    "This is some pretty messy data cleaning and for sure needs proper pre-filtering using data fields rather than just the text, but it works for this prototype ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e73b2532-dd62-4518-b76b-6712ed26402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26336"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of empty issues and any which are actually PRs\n",
    "non_empty = [x for x in data if len(x['body']) > 1]\n",
    "just_issues = [x for x in non_empty if not (len(x['pull_request']) > 0)]\n",
    "\n",
    "# This is all issues - opened and closed\n",
    "len(just_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88394d83-4c38-441e-b0cb-905da58f8ff1",
   "metadata": {},
   "source": [
    "Do you want to search just open issues or closed ones too? Set this to False if you want to search all previous issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b3a7fb3-2773-41a7-bee2-df62c32a3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_open = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c16a701f-3a37-4bbe-ac4b-69e676ecdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if just_open:\n",
    "    just_issues = [x for x in just_issues if x['state'] == \"open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "187eb59c-9102-470c-b04b-a907080c9fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4235"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(just_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "01845e93-d0d0-4b75-8d2a-9a3d6dffde8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to clean up the data\n",
    "\n",
    "# Remove code chunks from issue body\n",
    "def remove_code_chunks(text):\n",
    "    # Remove fenced code blocks (```...```)\n",
    "    text = re.sub(r\"```.*?\\n.*?```\", \"\", text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove inline code (`...`)\n",
    "    text = re.sub(r\"`[^`]*`\", \"\", text)\n",
    "    \n",
    "    # Remove indented code blocks (lines starting with 4+ spaces or a tab)\n",
    "    text = re.sub(r\"^(?: {4,}|\\t).*\\n?\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove URLs from issue body\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'(https?://\\S+|www\\.\\S+|ftp://\\S+)', '', text)\n",
    "\n",
    "# Remove boilerplate issue text\n",
    "def remove_boilerplate(text):\n",
    "\n",
    "    phrases_to_remove = [\n",
    "        \"### Describe the enhancement requested\",\n",
    "        \"### Describe the bug, including details regarding any error messages, version, and platform.\",\n",
    "        \"### Component(s)\",\n",
    "        \"### Describe the usage question you have. Please include as many useful details as  possible.\",\n",
    "        \"**_Overview_**\",\n",
    "        \"**_Impact_**\",\n",
    "        \"**_Key Features_**\"\n",
    "    ]\n",
    "    \n",
    "    for phrase in phrases_to_remove:\n",
    "        text = text.replace(phrase, '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "53e51ff1-f68d-4996-a1d3-f6f3fe2b457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in just_issues:\n",
    "    body = x.get('body', '')\n",
    "    if not isinstance(body, str):\n",
    "        continue  # or set x['body'] = \"\" if you prefer\n",
    "    x['body'] = remove_boilerplate(x['body'])\n",
    "    x['body'] = remove_code_chunks(x['body'])\n",
    "    x['body'] = remove_urls(x['body'])\n",
    "    x['body'] = \"\\n\".join(line for line in x['body'].splitlines() if line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd92070-879e-495b-94be-b688336c9ca9",
   "metadata": {},
   "source": [
    "And now to upload it to a searchable vector DB..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e9fac739-b359-496b-8f5f-c777657d27e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120990/2050716939.py:7: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "# Set up the vector DB\n",
    "\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2', device='cpu') # Model to create embeddings\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance\n",
    "\n",
    "# Create collection to store issues\n",
    "qdrant.create_collection(\n",
    "    collection_name=\"arrow_issues\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create embeddings and upload to collection\n",
    "qdrant.upload_points(\n",
    "    collection_name=\"arrow_issues\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=encoder.encode(doc[\"body\"]).tolist(),\n",
    "            payload=doc\n",
    "        ) for idx, doc in enumerate(just_issues) \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab5b62-53b4-4920-8c7f-08f00fb70ab2",
   "metadata": {},
   "source": [
    "Choose a term to search for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3923f732-0080-4fbc-b1f8-8bcbf957c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_term_to_search = \"compatability with pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47e5a49b-78cf-4f54-b09f-350c7fe7fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search!\n",
    "hits = qdrant.search(\n",
    "    collection_name=\"arrow_issues\",\n",
    "    query_vector=encoder.encode(my_term_to_search).tolist(),\n",
    "    limit=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e298ef85-e7f5-4e16-86b1-e13e96792d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://github.com/apache/arrow/issues/29025\n",
      "Score: 0.6416391935666161\n",
      "Currently we don't really explicitly define a \"minimum supported version\" for pandas, but we have (nightly) test builds with pandas 0.23 and 0.24 (in addition to latest and master) as oldest tested versions.\n",
      "I think we can bump the minimum support version (pandas 0.23 was first released May 15, 2018, so more than three years ago). We could maybe directly bump to pandas 1.0 (released January 29, 2020), or otherwise something in between (eg 0.25, released July 18, 2019).\n",
      "**Reporter**: [Joris Van den Bossche]( / @jorisvandenbossche\n",
      "#### Related issues:\n",
      "- [[Python] Remove backward compatibility hacks from pyarrow.pandas_compat]( (is related to)\n",
      "<sub>**Note**: *This issue was originally created as [ARROW-13351]( Please see the [migration documentation]( for further details.*</sub>\n",
      "=================================\n",
      "\n",
      "URL: https://github.com/apache/arrow/issues/44068\n",
      "Score: 0.6162408186901824\n",
      "This issue is to discuss the idea of moving a significant part of the pandas conversion and compatibility code to the pandas project itself. Of course we would keep all low-level conversions (e.g. everything that lives in C++) at the array-level, but a large part of  could live in pandas.\n",
      "Some reasons to do this:\n",
      "- It's a lot of pandas specific code that might \"fit\" better in pandas itself\n",
      "- It would allow pandas to control the conversion more tightly\n",
      "  - Example: now with upcoming pandas 3.0 and the new string dtype, pandas could ensure to use that new dtype in any conversion, while now with older versions of pyarrow  will still give object dtype (\n",
      "- The required low-level functionality in pyarrow should now also be stable enough to allow having this code live in pandas itself (which might not have been the case at the inception of pyarrow)\n",
      "- (it would be a good reason to clean up that code thoroughly, which it can use ..)\n",
      "A potential downside is that it makes the dependency structure even more complex (pyarrow's  relying on pandas relying on pyarrow), although we already have infrastructure set up to lazily import pandas.\n",
      "The idea is not that we would change any public pyarrow API that supports pandas (ingesting pandas in various constructors,  methods on objects), but that at least for the DataFrame and Series level, we under the hood rely on a method from pandas to do that conversion. \n",
      "For example, I think that most of the handling of the \"pandas metadata\" (to guarantee a better pandas <-> arrow roundtrip) could live in pandas itself.\n",
      "Eventually that would allow us to remove a lot of this pandas compatibility code from pyarrow, but note that this is very much a long term goal as we will need to keep that code around until we drop support for all pandas versions older than the version that would add this functionality to pandas.  \n",
      "(so that is another downside, that short term it might increase maintenance effort because of a version of that code living in two places)\n",
      "Equivalent issue on the pandas side: \n",
      "=================================\n",
      "\n",
      "URL: https://github.com/apache/arrow/issues/36035\n",
      "Score: 0.597528611680743\n",
      "Since the pandas unit was nanoseconds, would be nice if that were preserved in the inferred type, without having to specify ns explicitly, as microseconds results in precision loss.\n",
      "Python\n",
      "=================================\n",
      "\n",
      "URL: https://github.com/apache/arrow/issues/35802\n",
      "Score: 0.5793968280088357\n",
      "As pandas 2.0 start supporting pyarrow as engine and dtype.\n",
      "If we can support it in [pyarrow.Table.to_pandas()]( will be really helpful and efficient.\n",
      "Python\n",
      "=================================\n",
      "\n",
      "URL: https://github.com/apache/arrow/issues/27102\n",
      "Score: 0.5626824115990378\n",
      "We might need dictionary type support in order to process categorical types in Pandas correctly.\n",
      "**Reporter**: [Ian Alexander Joiner]( / @iajoiner\n",
      "<sub>**Note**: *This issue was originally created as [ARROW-11197]( Please see the [migration documentation]( for further details.*</sub>\n",
      "=================================\n",
      "\n",
      "URL: https://github.com/apache/arrow/issues/34070\n",
      "Score: 0.5613506415963849\n",
      "Using the  dtype prevents the usage of .\n",
      "pandas : 2.0.0.dev0+1430.gb2a26ecc5\n",
      "numpy : 1.23.4\n",
      "pyarrow : 9.0.0\n",
      "OS : Darwin\n",
      "OS-release : 21.6.0\n",
      "Version : Darwin Kernel Version 21.6.0: Mon Aug 22 20:17:10 PDT 2022; root:xnu-8020.140.49~2/RELEASE_X86_64\n",
      "machine : x86_64\n",
      "processor : i386\n",
      "Python\n",
      "=================================\n",
      "\n",
      "URL: https://github.com/apache/arrow/issues/38805\n",
      "Score: 0.5556338489025581\n",
      "Is type duration[pyarrow] supported in pandas?\n",
      "This piece of code creates pd.DataFrame with column \"a\" of type duration[pyarrow]:\n",
      "But this line:\n",
      "raises exception:\n",
      "Is it a bug or  limited support of arrow types in pandas for now? \n",
      "Python\n",
      "=================================\n",
      "\n",
      "URL: https://github.com/apache/arrow/issues/43031\n",
      "Score: 0.5389329165107454\n",
      "Cross post from \n",
      "- python 3.11\n",
      "- pyarrow 16.1.0\n",
      "- pandas 2.2.2\n",
      "- numpy 2.0.0\n",
      "Python\n",
      "=================================\n",
      "\n",
      "URL: https://github.com/apache/arrow/issues/46151\n",
      "Score: 0.5343986085660597\n",
      "I was doing something like this:\n",
      "This failed with:\n",
      "This is quite strange, because I hoped that pyarrow does not depend on pandas / pandas installed correctly. And it was strange that this threw only at  call, and even before checking if file existed.\n",
      "I thought that pandas depended on pyarrow and not in reverse. Is it not true? And in any case, some explicit error message with minimal required pandas version might be more clear to the user.\n",
      "This was with: \n",
      "After I did  and reinstalled again (got ), the problem fixed itself, and became to error-out at the non-existing file.\n",
      "Thanks!\n",
      "Python\n",
      "=================================\n",
      "\n",
      "URL: https://github.com/apache/arrow/issues/41496\n",
      "Score: 0.5330504532452462\n",
      "#39968, which is included in version 16.0.0 release, added a Python binding of C++  to the PyArrow API.\n",
      "However, (1) this addition of the native file system implementation for Azure has not yet been documented, and (2) this addition causes a backward compatibility issue in Pandas.\n",
      "### Documentation of the API and usage\n",
      "We should document  in the [API reference]( and its usage in the [user guide](\n",
      "### Note about the backward compatibility in Pandas\n",
      "Pandas'  and  with  have stopped working in specific cases since PyArrow 16.0.0 due to the addition of  in PyArrow.\n",
      "Pandas implement a logic that first tries to get a PyArrow native file system implementation for a given URL and then falls back to fsspec if PyArrow does not have a native implementation for the URL.\n",
      "Due to this fallback logic, Pandas's  and  always use fssepc with PyArrow before 16.0.0.\n",
      "With PyArrow 16.0.0, Pandas automatically uses PyArrow's native . However, this  does not use authentication settings set in fsspec's global configuration. Instead, we must explicitly provide an authentication setting to  and  as  independently of fsspec.\n",
      "We need to figure out where and how we should document this backward compatibility issue.\n",
      "Python\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for hit in hits:\n",
    "    print(f\"URL: {hit.payload['url']}\\nScore: {hit.score}\\n{hit.payload['body']}\\n=================================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
