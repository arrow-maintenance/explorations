{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176b0ef7-4ef1-41fd-87b3-b6327e52c76a",
   "metadata": {},
   "source": [
    "# Vector DB Search \n",
    "\n",
    "This notebook can be used to do a vector DB search of Arrow issues.  This allows for semantic search—retrieving issues based on meaning rather than exact keyword matches. This differs from GitHub's built-in search, which is mostly lexical and relies on specific terms, labels, or filters. A vector DB can surface issues with similar intent or topic even if they use different wording, making it more useful for detecting duplicates, related bugs, or thematic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "582fab66-3adf-4e9f-b9b6-c038d100073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "785cb46b-04ee-4dc2-8d1d-e0cca0cf5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"../test_data/issues_min.json.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    df = json.load(f)\n",
    "    \n",
    "df = pd.DataFrame(df)\n",
    "data = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f2de7-915b-46ec-981f-e33ac46f32c0",
   "metadata": {},
   "source": [
    "This is some pretty messy data cleaning and for sure needs proper pre-filtering using data fields rather than just the text, but it works for this prototype ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73b2532-dd62-4518-b76b-6712ed26402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26349"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of empty issues and any which are actually PRs\n",
    "non_empty = [x for x in data if len(x['body']) > 1]\n",
    "just_issues = [x for x in non_empty if not (len(x['pull_request']) > 0)]\n",
    "\n",
    "# This is all issues - opened and closed\n",
    "len(just_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88394d83-4c38-441e-b0cb-905da58f8ff1",
   "metadata": {},
   "source": [
    "Do you want to search just open issues or closed ones too? Set this to False if you want to search all previous issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b3a7fb3-2773-41a7-bee2-df62c32a3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_open = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c16a701f-3a37-4bbe-ac4b-69e676ecdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if just_open:\n",
    "    just_issues = [x for x in just_issues if x['state'] == \"open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187eb59c-9102-470c-b04b-a907080c9fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4236"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(just_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01845e93-d0d0-4b75-8d2a-9a3d6dffde8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to clean up the data\n",
    "\n",
    "# Remove code chunks from issue body\n",
    "def remove_code_chunks(text):\n",
    "    # Remove fenced code blocks (```...```)\n",
    "    text = re.sub(r\"```.*?\\n.*?```\", \"\", text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove inline code (`...`)\n",
    "    text = re.sub(r\"`[^`]*`\", \"\", text)\n",
    "    \n",
    "    # Remove indented code blocks (lines starting with 4+ spaces or a tab)\n",
    "    text = re.sub(r\"^(?: {4,}|\\t).*\\n?\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove URLs from issue body\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'(https?://\\S+|www\\.\\S+|ftp://\\S+)', '', text)\n",
    "\n",
    "# Remove boilerplate issue text\n",
    "def remove_boilerplate(text):\n",
    "\n",
    "    phrases_to_remove = [\n",
    "        \"### Describe the enhancement requested\",\n",
    "        \"### Describe the bug, including details regarding any error messages, version, and platform.\",\n",
    "        \"### Component(s)\",\n",
    "        \"### Describe the usage question you have. Please include as many useful details as  possible.\",\n",
    "        \"**_Overview_**\",\n",
    "        \"**_Impact_**\",\n",
    "        \"**_Key Features_**\"\n",
    "    ]\n",
    "    \n",
    "    for phrase in phrases_to_remove:\n",
    "        text = text.replace(phrase, '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e51ff1-f68d-4996-a1d3-f6f3fe2b457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in just_issues:\n",
    "    body = x.get('body', '')\n",
    "    if not isinstance(body, str):\n",
    "        continue  # or set x['body'] = \"\" if you prefer\n",
    "    x['body'] = remove_boilerplate(x['body'])\n",
    "    x['body'] = remove_code_chunks(x['body'])\n",
    "    x['body'] = remove_urls(x['body'])\n",
    "    x['body'] = \"\\n\".join(line for line in x['body'].splitlines() if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fbdac80-0306-4c27-9859-8f31ce7b7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fragment issues bodies into sentences\n",
    "\n",
    "import spacy\n",
    "# spacy.cli.download('en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "fragments = []\n",
    "for issue_id, issue in enumerate(just_issues):\n",
    "    doc = nlp(issue['body'])\n",
    "    for idx, sent in enumerate(doc.sents):\n",
    "        fragments.append({\n",
    "            \"fragment_id\": f\"{issue_id}_{idx}\",\n",
    "            \"text\": sent.text.strip(),\n",
    "            \"issue_id\": issue_id,\n",
    "            \"issue\": issue\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65bd78-4bf7-458f-b9c6-23c2f0fec84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf35068-49d9-431c-b416-732839eb53f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fd92070-879e-495b-94be-b688336c9ca9",
   "metadata": {},
   "source": [
    "And now to upload it to a searchable vector DB..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9fac739-b359-496b-8f5f-c777657d27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the vector DB\n",
    "\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2', device='cpu') # Model to create embeddings\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance\n",
    "\n",
    "# Create collection to store issues\n",
    "qdrant.create_collection(\n",
    "    collection_name=\"arrow_issues\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "qdrant.upload_points(\n",
    "    collection_name=\"arrow_issues\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=encoder.encode(fragment[\"text\"]).tolist(),\n",
    "            payload={\n",
    "                \"text\": fragment[\"text\"],\n",
    "                \"issue_id\": fragment[\"issue_id\"],\n",
    "                \"url\": fragment[\"issue\"][\"url\"],\n",
    "                \"title\": fragment[\"issue\"][\"title\"],\n",
    "                \"body\": fragment[\"issue\"][\"body\"]\n",
    "            }\n",
    "        ) for idx, fragment in enumerate(fragments)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab5b62-53b4-4920-8c7f-08f00fb70ab2",
   "metadata": {},
   "source": [
    "Choose a term to search for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3923f732-0080-4fbc-b1f8-8bcbf957c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_term_to_search = \"billions of rows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47e5a49b-78cf-4f54-b09f-350c7fe7fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search!\n",
    "hits = qdrant.search(\n",
    "    collection_name=\"arrow_issues\",\n",
    "    query_vector=encoder.encode(my_term_to_search).tolist(),\n",
    "    limit=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1e92f9b-76ac-4222-90a9-e8cb60f0f2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.7 <br>\n",
       "    <b>Matched Sentence:</b> I would like to write row-groups of ~9 million.<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/45018\">Is there a max limit to PyArrow write_dataset row-group sizes</a><br>\n",
       "    <b>Full Body:</b> NOTE:  I posted this to StackOverflow a few days ago, reposting here for more focused attention of the Arrow team.  Apologies if this breaches protocol.\n",
       "My code and observations are based on  version .\n",
       "**Question**: is there some internally imposed upper limit on the  parameter of ?\n",
       "I have a number of parquet files that I am trying to coalesce into larger files with larger row-groups.\n",
       "#### With write_dataset (doesn't work as expected)\n",
       "To do so, I am iterating over the files and using  to yield record batches, which form the input to . I would like to write row-groups of ~9 million.\n",
       "**Notes:**\n",
       "-  is from  and is basically the equivalent of pathlib.Path but works for both local as well as cloud storage.\n",
       "-  is set to ~9.1M\n",
       "-  is a smaller value, ~ 12k, to lower memory footprint while reading.\n",
       "When I check my output (using ) I see that I have a single file (as desired) but row-groups are no larger than ~2.1M.\n",
       "#### Alternative approach (works as expected)\n",
       "I then tried this alternative approach to coalescing the row-groups (adapted from [this gist](\n",
       " remains the same as before. The key difference here is that I am manually combining row-groups issued by , using .\n",
       "My output file has one row-group of ~9M, and a second row-group of ~3M, which is what I expect and want.\n",
       "So my question is... why does  not write up to the requested value of  but seem to cap off at ~2.1M?\n",
       "Parquet, Python <br>\n",
       "    <br>\n",
       "    \n",
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.67 <br>\n",
       "    <b>Matched Sentence:</b> Each fragment has tens of columns and tens of millions of rows.<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/31448\">Memory leak in `fragment.to_table`</a><br>\n",
       "    <b>Full Body:</b> This \"pseudo\" code ends with OOM.\n",
       "What is really weird is if we put a debug point in the loop and **load** just {**}one fragment{**}. It loads, but something **keeps eating memory after load** until there is no left.\n",
       "We are trying to read a parquet table that has several files under desired partitions. Each fragment has tens of columns and tens of millions of rows.\n",
       "**Reporter**: [ondrej metelka](\n",
       "<sub>**Note**: *This issue was originally created as [ARROW-16028]( Please see the [migration documentation]( for further details.*</sub> <br>\n",
       "    <br>\n",
       "    \n",
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.66 <br>\n",
       "    <b>Matched Sentence:</b> With increasing amount of such rows memory footprint overhead diminishes, but I want to focus on this specific case.<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/23592\">[Python] High memory usage writing pyarrow.Table with large strings to parquet</a><br>\n",
       "    <b>Full Body:</b> My case of datasets stored is specific. I have large strings (1-100MB each).\n",
       "Let's take for example a single row.\n",
       "43mb.csv is a 1-row CSV with 10 columns. One column a 43mb string.\n",
       "When I read this csv with pandas and then dump to parquet, my script consumes 10x of the 43mb.\n",
       "With increasing amount of such rows memory footprint overhead diminishes, but I want to focus on this specific case.\n",
       "Here's the footprint after running using memory profiler:\n",
       "Is this typical for parquet in case of big strings?\n",
       "**Environment**: Mac OSX\n",
       "**Reporter**: [Bogdan Klichuk](\n",
       "#### Related issues:\n",
       "- [[C++] Research jemalloc memory page reclamation configuration on macOS when background_thread option is unavailable]( (relates to)\n",
       "#### Original Issue Attachments:\n",
       "- [50mb.csv.gz](\n",
       "<sub>**Note**: *This issue was originally created as [ARROW-7305]( Please see the [migration documentation]( for further details.*</sub> <br>\n",
       "    <br>\n",
       "    \n",
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.65 <br>\n",
       "    <b>Matched Sentence:</b> That particular object was an  object with about 20 million rows and 26 columns.<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/28293\">[R] Writing to Parquet from tibble Consumes Large Amount of Memory</a><br>\n",
       "    <b>Full Body:</b> When writing a large  to a parquet file, a large amount of memory is consumed. I first discovered this when using  to load in an object that had been saved in the parquet format. That particular object was an  object with about 20 million rows and 26 columns. For a 5-6 GB object, memory ballooned by 22 GB.\n",
       "I wrote the following code to test this using a regular , not . In this test memory increases dramatically when writing, but not when reading, which I'm still trying to figure out.\n",
       "**Reporter**: [Jared Lander](\n",
       "<sub>**Note**: *This issue was originally created as [ARROW-12529]( Please see the [migration documentation]( for further details.*</sub> <br>\n",
       "    <br>\n",
       "    \n",
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.65 <br>\n",
       "    <b>Matched Sentence:</b> **We recommend large row groups (512MB - 1GB)**.<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/21090\">[C++][Parquet] Denominate row group size in bytes (not in no of rows)</a><br>\n",
       "    <b>Full Body:</b> Both the C++ [implementation of parquet writer for arrow]( and the [Python code bound to it]( appears denominated in the **number of rows** (without making it very explicit). Whereas:\n",
       "(1) [The Apache parquet documentation]( states: \n",
       "\"_Row group size: Larger row groups allow for larger column chunks which makes it possible to do larger sequential IO. Larger groups also require more buffering in the write path (or a two pass write). **We recommend large row groups (512MB - 1GB)**. Since an entire row group might need to be read, we want it to completely fit on one HDFS block. Therefore, HDFS block sizes should also be set to be larger. An optimized read setup would be: 1GB row groups, 1GB HDFS block size, 1 HDFS block per HDFS file._\"\n",
       "(2) Reference Apache [parquet-mr implementation]( for Java accepts the row size expressed in bytes.\n",
       "(3) The [low-level parquet read-write example]( also considers row group be denominated in bytes.\n",
       "These insights make me conclude that:\n",
       "- Per parquet design and to take advantage of HDFS block level operations, it only makes sense to work with row group sizes as expressed in bytes - as that is the only consequential desire the caller can utter and want to influence.\n",
       "- Arrow implementation of ParquetWriter would benefit from re-nominating its  into bytes. I will also note it is impossible to use pyarrow to shape equally byte-sized row groups as the size the row group takes is post-compression and the caller only know how much uncompressed data they have managed to put in.\n",
       "  Now, my conclusions can be wrong and I may be blind to some alley of reasoning, so this ticket is more of a question than a bug. A question on whether the audience here agrees with my reasoning and if not - to explain what detail I have missed.\n",
       "**Reporter**: [Remek Zajac](\n",
       "<sub>**Note**: *This issue was originally created as [ARROW-4542]( Please see the [migration documentation]( for further details.*</sub> <br>\n",
       "    <br>\n",
       "    \n",
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.63 <br>\n",
       "    <b>Matched Sentence:</b> Considering 1.000.000.000 records that's  rows per partition.<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/38691\">[C++][Parquet] splitting and saving big datasets consumes all available RAM and fails</a><br>\n",
       "    <b>Full Body:</b> I have a dataset stored as  files, 1000 files, 1.000.000 records each. Id like to convert it to  and split into logical partitions to save some space and ease access to it. However, when I try to do this my code consumes *all* available memory and fails spectacularly. \n",
       "The code to get mock data is provided below. The actual values are a bit different in terms of distributions, but not to the point of orders of magnitude. \n",
       "I'd like to split this data the following way:\n",
       "That's 600 partitions, give or take. Considering 1.000.000.000 records that's  rows per partition. Strictly speaking, I'd like to decrease this number even further by splitting it into 50-100 subsets since this is only a sample of data. \n",
       "I tried both Python and R to do this, the outcome stays the same. I also tried multiple machines with the following  specs: . . , no result so far. \n",
       "The only \"working\" solution is to read the whole dataset, filter a subset manually and save it. Wash, rinse, repeat; I think I had to read about 4TB of data in total during that, it also took somewhere around 10 hours to finish. This is clearly not sustainable.\n",
       "I also tried searching across other performance related issues, but most if not all of them are about *reading* data.\n",
       "No, I'm afraid I can't reduce the number of partitions, the only direction here is to go even further. When you do cross-table matching or targeted selection even these chunks are too big.\n",
       "C++, Parquet <br>\n",
       "    <br>\n",
       "    \n",
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.63 <br>\n",
       "    <b>Matched Sentence:</b> It is ~24K rows with 11 columns of which 8 are  like below and three are .<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/44942\">[Python][Parquet] reading parquet file saved with lz4 codec throws zstd error</a><br>\n",
       "    <b>Full Body:</b> I am experiencing inconsistent behaviour reading parquet files.  I have series of parquet files all from the same source using _lz4_ compression.  Using pandas and pyarrow in a very simple conda environment in both a MacOS and Ubuntu I have tried reading the files.  For most files there is no problem but for some files an error is thrown.\n",
       "For example, I have an unexceptional file that looks like this.  It is ~24K rows with 11 columns of which 8 are  like below and three are .\n",
       "In first case (MacOS) I can read the file, in the second case (Ubuntu) I can't.   What is very strange about the latter case is the error is for a different codec, _zstd_ and not _lz4_ as the error below shows.\n",
       "To replicate the conda env I'm using:\n",
       "The resulting env should be:\n",
       "The OS's are:  MacOS 15.0 (24A335) and Ubuntu 22.04.4 LTS\n",
       "The files were created using the polars [polars.LazyFrame.sink_parquet()]( because some files are larger than memory.  \n",
       "I have used both the 'zstd' and 'lz4' and can confirm that the files were saved with the 'lz4' codec.\n",
       "I can also open the file in VSCODE with the hex editor extension w/o a problem.\n",
       "Parquet, Python <br>\n",
       "    <br>\n",
       "    \n",
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.63 <br>\n",
       "    <b>Matched Sentence:</b> MAX_ROWS)<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/32384\">Unicode character issue with pyarrow</a><br>\n",
       "    <b>Full Body:</b> When running code using databricks SQL connector for Python, it hit a unicode character issue in pyarrow library. The customer has to put a workaround in the client code, something like\n",
       "\"SELECT decode(string(unbase64(value)), 'utf8')\"\n",
       "Exception in the main script No data fetched using SQL-statement: SELECT \\* FROM parquet.. Exception: Unknown error: Wrapping TP H�kan  Sweater failed Traceback (most recent call last):  \n",
       "File \"/home/xxxx/yy/allo/yy/db/sql_reader.py\", line 53, in query     rows = cursor.fetchmany(self.MAX_ROWS)  \n",
       "File \"/home/xxxx/yy/.venv/lib/python3.10/site-packages/databricks/sql/client.py\", line 401, in fetchmany     return self.active_result_set.fetchmany(size)  \n",
       "File \"/home/xxxx/yy/.venv/lib/python3.10/site-packages/databricks/sql/client.py\", line 630, in fetchmany     return self._convert_arrow_table(self.fetchmany_arrow(size))  \n",
       "File \"/home/xxxx/yy/.venv/lib/python3.10/site-packages/databricks/sql/client.py\", line 563, in _convert_arrow_table     df = table_renamed.to_pandas(  \n",
       "File \"pyarrow/array.pxi\", line 822, in pyarrow.lib._PandasConvertible.to_pandas  \n",
       "File \"pyarrow/table.pxi\", line 3889, in pyarrow.lib.Table._to_pandas  \n",
       "File \"/home/xxxx/yy/.venv/lib/python3.10/site-packages/pyarrow/pandas_compat.py\", line 803, in table_to_blockmanager     blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)  \n",
       "File \"/home/xxxx/yy/.venv/lib/python3.10/site-packages/pyarrow/pandas_compat.py\", line 1155, in _table_to_blocks     return [_reconstruct_block(item, columns, extension_columns)  \n",
       "File \"/home/xxxx/yy/.venv/lib/python3.10/site-packages/pyarrow/pandas_compat.py\", line 1155, in <listcomp>     return [_reconstruct_block(item, columns, extension_columns)  \n",
       "File \"/home/xxxx/yy/.venv/lib/python3.10/site-packages/pyarrow/pandas_compat.py\", line 763, in _reconstruct_block     pd_ext_arr = pandas_dtype.__from_arrow__(arr)  \n",
       "File \"/home/xxxx/yy/.venv/lib/python3.10/site-packages/pandas/core/arrays/string_.py\", line 217, in __from_arrow__     str_arr = StringArray._from_sequence(np.array(arr))  \n",
       "File \"pyarrow/array.pxi\", line 1395, in pyarrow.lib.Array.__array__  \n",
       "File \"pyarrow/array.pxi\", line 1441, in pyarrow.lib.Array.to_numpy  \n",
       "File \"pyarrow/error.pxi\", line 138, in pyarrow.lib.check_status pyarrow.lib.ArrowException: Unknown error: Wrapping TP H�kan  Sweater failed During handling of the above exception, another exception occurred: Traceback (most recent call last):  \n",
       "**Reporter**: [Yunbo Deng](\n",
       "<sub>**Note**: *This issue was originally created as [ARROW-17077]( Please see the [migration documentation]( for further details.*</sub> <br>\n",
       "    <br>\n",
       "    \n",
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.62 <br>\n",
       "    <b>Matched Sentence:</b> The memory regions MUST be large enough to fit this number of rows.<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/43762\">Proposal: generic streaming protocol for columnar data</a><br>\n",
       "    <b>Full Body:</b> I have a nagging feeling that the family of Arrow protocols and formats misses a generic protocol for streaming columnar data across ArrowArrays, such as to pack data from multiple arrays into a contiguous chunk of GPU memory. The goal is similar to [Dissociated IPC]( but I think a more flexible design is possible.\n",
       "The times and places where the concerns of Dissociated IPC are relevant is also when asynchronous programming model is often used (such as, because GPU operations are asynchronous with CPU), and therefore I think it makes sense to consider asynchronous streaming with cancels and back pressure, as well as off-loading a-la Dissociated IPC in a single protocol design space.\n",
       "Dissociated IPC’s  signal semantically always coincides with other async streaming signals, as will be detailed below, thus a separate type of message is not needed.\n",
       "Please don't take the description below too literally, it's more of a directional sketch and the invitation for discussion than a concrete proposal.\n",
       "Also, this is inspired by @westonpace's \"[columnar container format]( and can be seen as an attempt to do the same, but \"for the wire/transfer\" rather than \"for the disk/storage\".\n",
       "## Protocol description\n",
       "I think it makes the most sense to mostly inherit reactive streaming semantics from [RSocket]( specifically the semantics of [ERROR]( [CANCEL]( [LEASE]( [REQUEST_N]( and COMPLETE signals, indicating and how they interact with payload streaming: see [Request Stream sequences](\n",
       "Array streaming warrants one modification to RSocket semantics, however. RSocket’s flow control is defined in terms of *payload count* that Requester (I call it Receiver below) requests from Responder (I call it Sender) using [REQUEST_N]( signal. However, the most reasonable unit for flow control in ArrowArray streaming is the number of records/rows, and there will typically be many rows in a single payload.\n",
       "### Data plane characteristics negotiation\n",
       "Before array streaming begins, Sender and Receiver should negotiate the data plane concerns to minimise copies and optimise for the array production that precedes sending and the array consumption that follows reception, respectively.\n",
       "**Transfer modes**\n",
       "**1. Standard (in-band)**: the data transfer happens over the same transport as negotiation and control messaging. This is the default for Arrow Flight. This default transport will typically be gRPC, QUIC, or WebTransport.\n",
       "**2. Off-loaded Receiver read**: Sender sends to Receiver the addresses of memory regions (in its CPU or GPU) containing the array buffers via a PAYLOAD message. Receiver then reads the memory regions via out-of-band transfer. Sender assumes that the memory regions can be freed (or RDMA unpin, etc.) after Receiver sends any subsequent signal to Sender: either ERROR, CANCEL, REQUEST_N (which implies that the Receiver completed processing of this payload and ready to receive the next), or COMPLETE.\n",
       "Note: This mode is optimal for operation pipelining on Receiver’s side. Receiver’s GPU can pipeline processing operations with RDMA reads in CUDA. This is the mode is supported in Dissociated IPC. However, this mode “breaks the pipeline” on Sender’s side.\n",
       "When Receiver doesn’t have RDMA access to Sender’s memory, but they are still connected by some transport faster than UDP (or multiple transports in parallel), this mode is still favourable to sending the Array buffers in-band, albeit there won’t be any special win in Receiver’s GPU pipelining.\n",
       "**3. Off-loaded Sender write**: Receiver sends to Sender the addresses of memory regions in its CPU or GPU memory via REQUEST_N message (alongside the requested number of rows). The memory regions MUST be large enough to fit this number of rows.\n",
       "Then Sender writes array buffers to these memory regions via an out-of-band transfer. Sender can write fewer rows than than requested (it’s not guaranteed to have that many rows). Sender sends to Receiver the number of rows written via a subsequent PAYLOAD message.\n",
       "Upon receiving such a PAYLOAD message (as well as ERROR or COMPLETE signal), Receiver can take over the ownership of the memory regions back again and can free the resources that were needed to enable remote writing: free memory, RDMA unpin, etc.\n",
       "Note: This mode is optimal for pipelining on Sender’s side. Sender’s GPU can pipeline RDMA writes or  right after the processing operations that produce the Array buffers. However, RDMA writes are rarely available, so in practice it can mostly be used when Sender and Receiver are on the same PCIe bus, i.e., in the same node.\n",
       "**4. \"Simple\" off-loading**: Off-loading data plane to a different connection (a different port, and, perhaps, over a different transport, such as raw QUIC vs. gRPC) than the standard (in-band) connection, on which the control plane remains. There could perhaps be some performance or Array buffer alignment (see below) reasons to do this.\n",
       "Note: This is what Dissociated IPC also supports, by introducing the possibility of separate Data and Metadata streams, but Dissociated IPC makes this mode a different \"degree of freedom\" than the \"off-loaded Receiver read\" mode. I don't understand the need for this, and it looks unnecessarily over-complicated for me.\n",
       "**UCX or OFI/libfabric negotiation**\n",
       "Ideally, UCX or OFI/libfabric negotiation (rndv protocol) and connection establishment should happen only once per stream, together with the negotiation of Sender and Receiver which transfer mode they should use, rather than before each Array sent in the stream. \n",
       "**Batch sizes and alignment**\n",
       "Both Sender and Receiver may have minimum and/or optimal and/or maximum batch sizes. They should try agree to both use the batch size optimal for the slower side, and then Receiver requests this agreed-upon number of rows in each of its REQUEST_N messages.\n",
       "Receiver can specify that it has a minimum batch size for the off-loaded modes. If Sender has highly irregular Array sizes, it should accumulate them internally until it has prepared (e.g., read from disk) a sufficient number of rows for the minimum batch size. Only the trailing PAYLOAD may have insufficient rows, and it may need fall-back to the standard (in-band) mode.\n",
       "Finally, Receiver may have requirements regarding Array buffer alignment. If the standard (in-band) transport doesn’t guarantee frame’s data bytes alignment without an extra memory copy (i.e., the 64-byte alignment of the first buffer in the PAYLOAD message), Sender and Receiver can try to avoid this copy by establishing a separate connection (such as, on a different UDP port) with the transport protocol library configured to ensure such alignment.\n",
       "### Flow control\n",
       "Both Sender and Receiver must keep track of the total *cumulative* number of rows that Receiver requested via REQUEST_N messages since the opening of the stream and the total cumulative number of rows that Receiver has claimed to deliver via PAYLOAD messages.\n",
       "Sender must never send PAYLOADs that make the latter number (the number delivered rows) exceed the former (the number of requested rows).\n",
       "In the “Off-loaded Receiver read” mode, if Receiver needs to indicate to Sender that it has read rows from the memory regions (and thus the Sender can free the associated resources internally), but the Receiver is still fine with the remaining number of requested rows and doesn’t want to increase it, Receiver should send REQUEST_N message with n=0 and the addresses of \"consumed\" memory regions.\n",
       "In addition, in the “Off-loaded Sender write” mode, both Sender and Receiver must keep track of the sequences (one sequence for each Array buffer, to be precise) of Receiver’s memory regions that it offered via REQUEST_N messages, and the “high watermark” of written bytes in these memory regions. The high watermarks (one for each Array buffer) “flow through” these memory region sequences. Sender may also copy this information (watermarks) in PAYLOAD messages, to double-check with Receiver, mainly as a sanity measure.\n",
       "### “Multiplexing”: Array splitting into multiple streams\n",
       "I think it’s most reasonable that Sender and Receiver negotiate just one transfer mode per Array stream. If multiple modes are needed, Sender can split the Array into groups of buffers that need to be transferred in different modes, and then the original semantics “reconstructed” in the Receiver’s application.\n",
       "It’s also most likely that non-standard transfer modes (modes 2 and 3) can only work sanely for fixed-size layout buffers only. For variable-size layouts, it’s too complex to pack buffers from multiple subsequent Arrays into the same contiguous memory regions. I also don’t know what would be the use-cases for this, anyway.\n",
       "Thus, during the initial negotiation phase, Sender and Receiver can decide how they split Arrays into multiple buffer groups, and transfer them separately over different \"synthetic\" Array streams, possibly in different transfer modes. However, these Array streams should all share CANCEL, ERROR, COMPLETE, and REQUEST_N signals, happening on the standard (in-band) transport.\n",
       "The cumulative number of delivered rows (as claimed by Sender in PAYLOAD messages) in this case is assumed to be _the minimum such count across all streams_.\n",
       "It can be pointless to “splice” variable-size layout Array buffers that cover more rows than the number of rows in fixed-size layout buffers that Sender delivers in parallel. For example, in Run End Encoding layout, a single run may \"suddenly\" cover millions of rows (all having the same value in the corresponding column), while fixed-sized layout buffers are delivered in parallel in batches of just 64k. It makes sense to permit such variable-size layout buffers to “over-deliver” rows. The generality is not lost because, as noted in the previous paragraph, Sender and Receiver should track the minimum delivered rows across streams. If Receiver's application needs this, it can keep track of scan pointers within these variable-sized layout buffers and advance them in step with the minimum delivered row count.\n",
       "Note that semantically, this method of treating variable-sized layouts is close to DictionaryBatches, whose sizes are not predictable relative to the sizes of RecordBatches that they prepend or interleave, such as when the whole-stream DictionaryBatch with lots of rows may prepend a *smaller* first RecordBatch. Thus, DictionaryBatches can be transferred over their own separate synthetic stream.\n",
       "The generality is also preserved here, unless someone wants to deliver DictionaryBatches in modes 2 or 3, in which case they have to keep track of two different flow controls: one of the number of \"covered\" array rows (equal to the culumative number of rows in all Arrays up to and including the next Array in the \"original\" stream), and DictionaryBatches own records requested/delivered flow control. However, I'm not sure if there is any real need for delivering DictionaryBatches in modes 2 or 3.\n",
       "cc @zeroshade @raulcd @bkietz @pitrou @lidavidm\n",
       "FlightRPC, Format <br>\n",
       "    <br>\n",
       "    \n",
       "\n",
       "    <br>\n",
       "    <b>Score:</b> 0.61 <br>\n",
       "    <b>Matched Sentence:</b> Based on literature we might like to use batches somewhere in the range of 1k to 16k rows.<br>\n",
       "    <b>Issue URL:</b> <a target=\"blank\" href=\"https://github.com/apache/arrow/issues/31546\">[C++] Improve performance of ExecuteScalarExpression</a><br>\n",
       "    <b>Full Body:</b> One of the things we want to be able to do in the streaming execution engine is process data in small L2 sized batches.  Based on literature we might like to use batches somewhere in the range of 1k to 16k rows.  In ARROW-16014 we created a benchmark to measure the performance of ExecuteScalarExpression as the size of our batches got smaller.  There are two things we observed:\n",
       "- Something is causing thread contention.  We should be able to get pretty close to perfect linear speedup when we are evaluating scalar expressions and the batch size fits entirely into L2.  We are not seeing that.\n",
       "- The overhead of ExecuteScalarExpression is too high when processing small batches.  Even when the expression is doing real work (e.g. copies, comparisons) the execution time starts to be dominated by overhead when we have 10k sized batches.\n",
       "**Reporter**: [Weston Pace]( / @westonpace\n",
       "#### Subtasks:\n",
       "- [ ] [[C++] Overhead of std::shared_ptr<DataType> copies is causing thread contention](\n",
       "- [X] [[C++] Avoid copying shared_ptr in Expression::type()](\n",
       "- [X] [[C++] Avoid slicing array inputs in ExecBatchIterator that would result in one slice](\n",
       "- [X] [[C++] Implementation of ExecuteScalarExpressionOverhead benchmarks without arrow for comparision](\n",
       "#### Original Issue Attachments:\n",
       "- [Flamegraph.png](\n",
       "<sub>**Note**: *This issue was originally created as [ARROW-16138]( Please see the [migration documentation]( for further details.*</sub> <br>\n",
       "    <br>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_snippets = []\n",
    "\n",
    "for hit in hits:\n",
    "    block = f\"\"\"\n",
    "    <br>\n",
    "    <b>Score:</b> {round(hit.score, 2)} <br>\n",
    "    <b>Matched Sentence:</b> {hit.payload['text']}<br>\n",
    "    <b>Issue URL:</b> <a target=\"blank\" href=\"{hit.payload['url']}\">{hit.payload['title']}</a><br>\n",
    "    <b>Full Body:</b> {hit.payload['body']} <br>\n",
    "    <br>\n",
    "    \"\"\"\n",
    "    html_snippets.append(block)\n",
    "\n",
    "html_output = \"\\n\".join(html_snippets)\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298ef85-e7f5-4e16-86b1-e13e96792d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc8a6d-9e3b-494f-8656-c4e5fc3fbb65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
